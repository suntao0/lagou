# lagou
通过python爬虫利用requests模块爬获拉勾网数据下载并保存
 前言
本文从拉勾网爬取深圳市数据分析的职位信息，并以xls格式保存至电脑。

1. 用到的软件包
Python版本： Python3.6/3.7
requests: 下载网页
josn:分析并转换数据
time: 暂停进程
xlwt:数据以xls格式文件保存
2. 解析网页
打开Chrome,在拉勾网搜索某市的公司,使用检查功能查看网页源代码,发现拉勾网有反爬虫机制, 职位信息并不在源代码里,而是保存在JSON的文件里,因此我们直接下载JSON,并使用字典方法直接读取
数据.
抓取网页时,需要加上头部信息, 才能获取所需的数据.
3. 保存数据
通过爬取数据以后可以利用xls模块下载并保存下来了


#  反爬虫机制
破解反爬虫机制的几种方法

策略1：设置下载延迟，比如数字设置为5秒，越大越安全

策略2：禁止Cookie，某些网站会通过Cookie识别用户身份，禁用后使得服务器无法识别爬虫轨迹

策略3：使用user agent池。也就是每次发送的时候随机从池中选择不一样的浏览器头信息，防止暴露爬虫身份

策略4：使用IP池，这个需要大量的IP资源，可以通过抓取网上免费公开的IP建成自有的IP代理池。

策略5：分布式爬取，这个是针对大型爬虫系统的，实现一个分布式的爬虫，主要为以下几个步骤： 
1、基本的http抓取工具，如scrapy； 
2、避免重复抓取网页，如Bloom Filter； 
3、维护一个所有集群机器能够有效分享的分布式队列； 
4、将分布式队列和Scrapy的结合； 
5、后续处理，网页析取(如python-goose)，存储(如Mongodb)。

策略6：模拟登录—浏览器登录的爬取

设置一个cookie处理对象，它负责将cookie添加到http请求中，并能从http响应中得到cookie，向网站登录页面发送一个请求Request, 包括登录url，POST请求的数据，Http header利用urllib2.urlopen发送请求，接收WEB服务器的Response。
